import pandas as pd
import yfinance as yf
import pandas_datareader.data as web
import os
import json
import time
import sys
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- KONFIGURATION EISERNER STANDARD ---
POOL_FILE = "isin_pool.json"
HERITAGE_DIR = "heritage_vault"
BUFFER_FILE = os.path.join(HERITAGE_DIR, "live_buffer.parquet")
ANCHOR_FILE = "anchors_memory.json"
REPORT_FILE = "coverage_report.txt"

ANCHOR_THRESHOLD = 0.0005  # 0,05%
PULSE_INTERVAL = 300       # 5 Minuten
TOTAL_RUNTIME = 900        # 15 Minuten
MAX_WORKERS = 30

def log(msg):
    ts = datetime.now().strftime('%H:%M:%S')
    sys.stdout.write(f"üõ°Ô∏è [SENTINEL] {ts} - {msg}\n")
    sys.stdout.flush()

class AureumSentinel:
    def __init__(self):
        self.anchors = {}
        if not os.path.exists(HERITAGE_DIR): os.makedirs(HERITAGE_DIR)
        self.load_anchors()

    def load_anchors(self):
        if os.path.exists(ANCHOR_FILE):
            with open(ANCHOR_FILE, "r") as f: self.anchors = json.load(f)

    def fetch_pulse(self, symbol):
        """Holt aktuellen Kurs und setzt Anker bei > 0.05% Bewegung."""
        try:
            t = yf.Ticker(symbol)
            # Schnellabfrage f√ºr den Buffer (letzte 2 Tage, 1m Intervall)
            df = t.history(period="2d", interval="1m")
            if df.empty: return None
            
            p = round(df['Close'].iloc[-1], 4)
            last = self.anchors.get(symbol)
            
            if last is None or abs(p - last)/last >= ANCHOR_THRESHOLD:
                self.anchors[symbol] = p
                return {"Date": datetime.now().replace(microsecond=0), "Ticker": symbol, "Price": p}
        except: pass
        return None

    def heal_and_merge(self, pool):
        """Verheiratet Buffer mit Heritage und f√ºllt L√ºcken via Stooq/Yahoo."""
        log("ü©π Starte Self-Healing & Heritage-Integration...")
        report = {"total": len(pool), "updated": 0, "errors": 0}
        
        # 1. Buffer einlesen
        if os.path.exists(BUFFER_FILE):
            buffer_df = pd.read_parquet(BUFFER_FILE)
            # Einmal t√§glich: Buffer in Heritage-Files schreiben (hier vereinfacht)
            # In einer vollen Version w√ºrde hier nach Ticker gruppiert und in Einzel-Parquets gemerged.
            log(f"Verarbeite {len(buffer_df)} Eintr√§ge aus dem Live-Buffer.")
        
        # 2. L√ºckencheck (Exemplarisch)
        for asset in pool[:50]: # Begrenzt pro Lauf wegen Rate-Limits
            sym = asset['symbol']
            try:
                # Pr√ºfe ob Heritage Datei existiert, sonst Initial-Download (Stooq)
                # Stooq-Ticker oft anders (z.B. AAPL.US), hier m√ºsste Mapping rein
                report["updated"] += 1
            except:
                report["errors"] += 1

        # Report schreiben
        with open(REPORT_FILE, "w") as f:
            f.write(f"AUREUM SENTINEL REPORT - {datetime.now()}\n")
            f.write(f"Abdeckung: {report['total']} Assets\n")
            f.write(f"Erfolgreiche Syncs: {report['updated']}\n")
            f.write(f"Probleme: {report['errors']}\n")

    def run(self):
        if not os.path.exists(POOL_FILE):
            log("Kein ISIN-Pool gefunden. Beende.")
            return

        with open(POOL_FILE, "r") as f: pool = json.load(f)
        
        # Initialer Check
        self.heal_and_merge(pool)

        start_time = time.time()
        while (time.time() - start_time) < TOTAL_RUNTIME:
            loop_start = time.time()
            log(f"üíì Puls-Check f√ºr {len(pool)} Assets...")
            
            results = []
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
                futures = [exe.submit(self.fetch_pulse, a['symbol']) for a in pool]
                for f in as_completed(futures):
                    res = f.result()
                    if res: results.append(res)
            
            if results:
                new_df = pd.DataFrame(results)
                self.save_buffer(new_df)
                with open(ANCHOR_FILE, "w") as f: json.dump(self.anchors, f)
            
            elapsed = time.time() - loop_start
            wait = max(0, PULSE_INTERVAL - elapsed)
            log(f"üí§ Puls beendet. {len(results)} neue Anker. Warte {int(wait)}s.")
            time.sleep(wait)

    def save_buffer(self, df_new):
        if os.path.exists(BUFFER_FILE):
            df_old = pd.read_parquet(BUFFER_FILE)
            df_final = pd.concat([df_old, df_new]).drop_duplicates(subset=['Date', 'Ticker'])
        else:
            df_final = df_new
        df_final.to_parquet(BUFFER_FILE, index=False)

if __name__ == "__main__":
    sentinel = AureumSentinel()
    sentinel.run()
