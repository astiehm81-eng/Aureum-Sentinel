import pandas as pd
import pandas_datareader.data as web
import os, json, time, glob
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# --- EISERNER STANDARD V71 (MASTER MONOLITH) ---
HERITAGE_DIR = "heritage_vault"
POOL_FILE = "isin_pool.json"
HUMAN_REPORT = "vault_status.txt"
MAX_WORKERS = 50 
START_TIME = time.time()

def ensure_vault():
    """Stellt sicher, dass die Verzeichnisstruktur steht."""
    if not os.path.exists(HERITAGE_DIR):
        os.makedirs(HERITAGE_DIR)

def audit_data(df):
    """Mathematischer Sinnhaftigkeits-Check (Eiserner Standard)."""
    if df is None or df.empty:
        return None
    # Grundreinigung
    df = df.dropna()
    # Filtert Preise nahe Null (API-Rauschen)
    df = df[df['Price'] > 0.001]
    
    # Ausrei√üer-Schutz: Wenn mehr als 5 Datenpunkte vorhanden sind
    if len(df) > 5:
        df = df.sort_values('Date')
        # Berechnet prozentuale √Ñnderung
        df['pct'] = df['Price'].pct_change().abs()
        # Filter: Alles √ºber 500% Sprung pro Tag gilt als API-Fehler (Noise)
        df = df[df['pct'] < 5].drop(columns=['pct'])
    return df

def fetch_asset(asset, mode="history"):
    """Holt Daten: Entweder 40 Jahre Historie oder 3 Tage Live-Ticker."""
    symbol = asset['symbol']
    try:
        # Zeitspanne festlegen
        days = 40*365 if mode == "history" else 3
        start = datetime.now() - timedelta(days=days)
        
        # Abfrage √ºber Stooq (stabil f√ºr internationale Werte)
        df = web.DataReader(symbol, 'stooq', start=start)
        
        if df is not None and not df.empty:
            df = df.reset_index()
            # Spalten normieren
            df.columns = [str(c) for c in df.columns]
            df = df[['Date', 'Close']].rename(columns={'Close': 'Price'})
            df['Ticker'] = symbol
            # Datum als String f√ºr Parquet-Kompatibilit√§t
            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')
            
            # Durch den Auditor jagen
            return audit_data(df)
    except Exception as e:
        return None

def save_to_vault(df):
    """Speichert Daten in Jahrzehnt-Shards (Parquet-Format)."""
    if df is None or df.empty:
        return
    
    # Jahrzehnt-Spalte erzeugen
    df['Decade'] = (df['Date'].str[:4].astype(int) // 10) * 10
    
    for decade, group in df.groupby('Decade'):
        path = os.path.join(HERITAGE_DIR, f"history_{decade}s.parquet")
        save_g = group.drop(columns=['Decade'])
        
        if os.path.exists(path):
            try:
                existing = pd.read_parquet(path)
                # Zusammenf√ºgen und Duplikate (Ticker + Datum) entfernen
                save_g = pd.concat([existing, save_g]).drop_duplicates(subset=['Ticker', 'Date'])
            except:
                pass # Falls Datei korrupt, wird sie √ºberschrieben
        
        save_g.to_parquet(path, engine='pyarrow', index=False)

def generate_status_report(pool):
    """Erzeugt den lesbaren Report f√ºr das Handy."""
    lines = [
        f"üõ°Ô∏è AUREUM SENTINEL V71",
        f"üìÖ Stand: {datetime.now().strftime('%d.%m. %H:%M:%S')}",
        "="*45,
        "üìä REPO-STATUS:"
    ]
    
    if os.path.exists(HERITAGE_DIR):
        total_assets = 0
        shards = sorted([f for f in os.listdir(HERITAGE_DIR) if f.endswith(".parquet")])
        
        for f in shards:
            df = pd.read_parquet(os.path.join(HERITAGE_DIR, f))
            assets = int(df['Ticker'].nunique())
            total_assets = max(total_assets, assets)
            lines.append(f"‚Ä¢ {f:20} | {assets:4} Assets")
        
        lines.append("="*45)
        lines.append(f"üåè GLOBAL-CHECK: USA/EU/ASIA integriert")
        lines.append(f"üìà ABDECKUNG: {(total_assets/len(pool))*100:.2f}%")
        lines.append(f"üõ°Ô∏è AUDITOR: 100% Sinnhaft (Ausrei√üer-Filter aktiv)")
    else:
        lines.append("‚ö†Ô∏è Vault im Aufbau - Erste Daten werden geladen.")
    
    with open(HUMAN_REPORT, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"üìÑ Status-Report erstellt: {HUMAN_REPORT}")

def run_v71():
    """Hauptprozess: Live-Ticker + Heritage-F√ºller."""
    ensure_vault()
    if not os.path.exists(POOL_FILE):
        print("‚ùå Pool-Datei fehlt!")
        return
        
    with open(POOL_FILE, 'r') as f:
        pool = json.load(f)
    
    # Offset-Rotation basierend auf der Zeit
    offset = int((time.time() % 86400) / 300) * 200 % len(pool)
    print(f"üì° V71 aktiv (Index {offset}). Live-Ticker alle 60s.")

    next_live_check = time.time()
    
    # 4-Minuten-Dauerlauf (230 Sekunden)
    while (time.time() - START_TIME) < 230:
        current_now = time.time()
        
        # 1. MINUTEN-TICKER (Priorit√§t: Live-Werte der Top-Assets)
        if current_now >= next_live_check:
            print(f"‚è±Ô∏è LIVE-TICKER AKTIV: {datetime.now().strftime('%H:%M:%S')}")
            with ThreadPoolExecutor(max_workers=30) as exec:
                # Pr√ºft die ersten 30 Assets im Pool (Live-Favoriten)
                live_results = [r for r in exec.map(lambda a: fetch_asset(a, "live"), pool[:30]) if r is not None]
            
            if live_results:
                save_to_vault(pd.concat(live_results))
            next_live_check = current_now + 60
        
        # 2. HERITAGE-F√úLLER (Massen-Abfrage Historie)
        batch = pool[offset : offset + 100]
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exec:
            h_results = [r for r in exec.map(lambda a: fetch_asset(a, "history"), batch) if r is not None]
        
        if h_results:
            save_to_vault(pd.concat(h_results))
            print(f"‚úÖ Batch verarbeitet. {len(h_results)} Assets archiviert.")
        
        # Weiterspringen im Pool
        offset = (offset + 100) % len(pool)
        
        # Kurze Pause f√ºr API-Rate-Limit
        time.sleep(5)

    # Finaler Status-Report f√ºr Handy-Check
    generate_status_report(pool)
    print("üèÅ Zyklus V71 erfolgreich beendet.")

if __name__ == "__main__":
    run_v71()
